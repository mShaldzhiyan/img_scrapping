{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cloudscraper\n",
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_characters(s):\n",
    "    return re.sub(r'[^a-zA-Z0-9_]+', '_', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_to_dict(tile):\n",
    "    item = dict()\n",
    "    item['title'] = tile.findAll('a')[0]['title']\n",
    "    item['data-itemid'] = tile['data-itemid']\n",
    "    item['url'] = tile['data-monetate-producturl']\n",
    "    try:\n",
    "        item['colors'] = [a.find('img')['alt'] for a in tile.findAll('li')]\n",
    "    except Exception as e:\n",
    "        print(e, 'probably no colors')\n",
    "        item['colors'] = None\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PERSON = \"deep/person\"\n",
    "FOLDER_NO_PERSON = \"deep/no_person\"\n",
    "FOLDER_TEST = \"deep\"\n",
    "\n",
    "def save_images(img_urls, id):\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    for img_url in enumerate(img_urls):\n",
    "        response = scraper.get(img_url[1])\n",
    "        filename = f'{id}_{img_url[0]}.jpg'\n",
    "        if 'alternate' in img_url[1]:\n",
    "            dst_file = os.path.join(FOLDER_PERSON, filename)\n",
    "        elif 'lifestyle' in img_url[1]:\n",
    "            dst_file = os.path.join(FOLDER_NO_PERSON, filename)\n",
    "        else:\n",
    "            dst_file = os.path.join(FOLDER_TEST, filename)\n",
    "        with open(dst_file, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "\n",
    "def get_images(url: str, id: str, colors: list):  # add colors parse\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    if colors:\n",
    "        for a in colors:\n",
    "            colorurl = a.replace(' ', '%20')\n",
    "            colorurl = 'colorname=' + a\n",
    "            url = re.sub(r'colorname=([^&]+)(?=$|&)', colorurl, url)\n",
    "            response = scraper.get(url)\n",
    "            bs = BeautifulSoup(response.content, 'html.parser')\n",
    "            imgs = bs.findAll('picture', attrs={'class': 'swiper-zoomable'})\n",
    "            img_urls = [a['data-highres-images'] for a in imgs]\n",
    "            save_images(img_urls, id + '_' + replace_special_characters(a))\n",
    "    else:\n",
    "        response = scraper.get(url)\n",
    "        bs = BeautifulSoup(response.content, 'html.parser')\n",
    "        imgs = bs.findAll('picture', attrs={'class': 'swiper-zoomable'})\n",
    "        img_urls = [a['data-highres-images'] for a in imgs]\n",
    "        save_images(img_urls, id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images(url):\n",
    "    print('Getting items info')\n",
    "    tiles = []\n",
    "    for i in range(999): #could be replaced with while True, but it's not necessary\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        print(i*32)\n",
    "        url = url.replace('?', f'?start={i*32}&')\n",
    "        response = scraper.get(url)\n",
    "        if response.status_code == 403:\n",
    "            print('auth error')\n",
    "            break\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        product_tiles = soup.select('.product-tile')\n",
    "        t = [tile_to_dict(a) for a in product_tiles]\n",
    "        if len(t) == 0:\n",
    "            print('no tiles')\n",
    "            break\n",
    "        tiles.extend(t)\n",
    "    df = pd.DataFrame(tiles)\n",
    "    df = df.drop_duplicates(subset=['data-itemid'], keep='first').reset_index(drop=True)\n",
    "    df.to_pickle('df.pkl')\n",
    "    print('Saving images')\n",
    "    for a in tqdm(zip(df.url, df['data-itemid'], df.colors), total=len(df)):\n",
    "        get_images(a[0], a[1], a[2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images_frompos(df:pd.DataFrame, pos:int):\n",
    "    df = df.iloc[pos:,:]\n",
    "    for a in tqdm(zip(df.url, df['data-itemid'], df.colors), total=len(df)):\n",
    "        get_images(a[0], a[1], a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting items info\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "'NoneType' object is not subscriptable probably no colors\n",
      "128\n",
      "160\n",
      "'NoneType' object is not subscriptable probably no colors\n",
      "'NoneType' object is not subscriptable probably no colors\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "no tiles\n",
      "Saving images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 264/274 [54:23<02:03, 12.36s/it]  \n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.rlmedia.io', port=443): Max retries exceeded with url: /is/image/PoloGSI/s7-1473088_alternate10?$rl_pdp_mob_zoom$ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002CB3E149AE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dns_host, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mC:\\apps\\python310\\lib\\socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[0;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\urllib3\\connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000002CB3E149AE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    789\u001b[0m )\n\u001b[0;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.rlmedia.io', port=443): Max retries exceeded with url: /is/image/PoloGSI/s7-1473088_alternate10?$rl_pdp_mob_zoom$ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002CB3E149AE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m URL \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.ralphlauren.nl/en/men/clothing/hoodies-sweatshirts/10204?webcat=men\u001b[39m\u001b[39m%\u001b[39m\u001b[39m7Cclothing\u001b[39m\u001b[39m%\u001b[39m\u001b[39m7Cmen-clothing-hoodies-sweatshirts\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m scrape_images(URL)\n\u001b[0;32m      3\u001b[0m df\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mscrape_images\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSaving images\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m tqdm(\u001b[39mzip\u001b[39m(df\u001b[39m.\u001b[39murl, df[\u001b[39m'\u001b[39m\u001b[39mdata-itemid\u001b[39m\u001b[39m'\u001b[39m], df\u001b[39m.\u001b[39mcolors), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(df)):\n\u001b[1;32m---> 24\u001b[0m     get_images(a[\u001b[39m0\u001b[39;49m], a[\u001b[39m1\u001b[39;49m], a[\u001b[39m2\u001b[39;49m])\n\u001b[0;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m, in \u001b[0;36mget_images\u001b[1;34m(url, id, colors)\u001b[0m\n\u001b[0;32m     35\u001b[0m imgs \u001b[39m=\u001b[39m bs\u001b[39m.\u001b[39mfindAll(\u001b[39m'\u001b[39m\u001b[39mpicture\u001b[39m\u001b[39m'\u001b[39m, attrs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mswiper-zoomable\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     36\u001b[0m img_urls \u001b[39m=\u001b[39m [a[\u001b[39m'\u001b[39m\u001b[39mdata-highres-images\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m imgs]\n\u001b[1;32m---> 37\u001b[0m save_images(img_urls, \u001b[39mid\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36msave_images\u001b[1;34m(img_urls, id)\u001b[0m\n\u001b[0;32m      6\u001b[0m scraper \u001b[39m=\u001b[39m cloudscraper\u001b[39m.\u001b[39mcreate_scraper()\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m img_url \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(img_urls):\n\u001b[1;32m----> 8\u001b[0m     response \u001b[39m=\u001b[39m scraper\u001b[39m.\u001b[39;49mget(img_url[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m      9\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mimg_url[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39malternate\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m img_url[\u001b[39m1\u001b[39m]:\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\requests\\sessions.py:600\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \n\u001b[0;32m    594\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[39m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    599\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 600\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(\u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\cloudscraper\\__init__.py:257\u001b[0m, in \u001b[0;36mCloudScraper.request\u001b[1;34m(self, method, url, *args, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m     (method, url, args, kwargs) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequestPreHook(\n\u001b[0;32m    245\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    246\u001b[0m         method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    250\u001b[0m     )\n\u001b[0;32m    252\u001b[0m \u001b[39m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39m# Make the request via requests.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[39m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m    256\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodeBrotli(\n\u001b[1;32m--> 257\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperform_request(method, url, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    258\u001b[0m )\n\u001b[0;32m    260\u001b[0m \u001b[39m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39m# Debug the request via the Response object.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug:\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\cloudscraper\\__init__.py:190\u001b[0m, in \u001b[0;36mCloudScraper.perform_request\u001b[1;34m(self, method, url, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_request\u001b[39m(\u001b[39mself\u001b[39m, method, url, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(CloudScraper, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mrequest(method, url, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\code\\python\\test_task\\scraping\\lib\\site-packages\\requests\\adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    562\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.rlmedia.io', port=443): Max retries exceeded with url: /is/image/PoloGSI/s7-1473088_alternate10?$rl_pdp_mob_zoom$ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002CB3E149AE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "URL = r\"https://www.ralphlauren.nl/en/men/clothing/hoodies-sweatshirts/10204?webcat=men%7Cclothing%7Cmen-clothing-hoodies-sweatshirts\"\n",
    "df = scrape_images(URL)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:43<00:00,  9.45s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('df.pkl')\n",
    "scrape_images_frompos(df, 263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>data-itemid</th>\n",
       "      <th>url</th>\n",
       "      <th>colors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Triple-Pony Plaid-Print Fleece Hoodie</td>\n",
       "      <td>624792</td>\n",
       "      <td>https://www.ralphlauren.nl/en/triple-pony-plai...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polo Ski Fleece Hoodie</td>\n",
       "      <td>624775</td>\n",
       "      <td>https://www.ralphlauren.nl/en/polo-ski-fleece-...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The RL Fleece Hoodie</td>\n",
       "      <td>490877v1</td>\n",
       "      <td>https://www.ralphlauren.nl/en/the-rl-fleece-ho...</td>\n",
       "      <td>[Holiday Red, Athletic Green, Harrison Blue]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Double-Knit Pullover</td>\n",
       "      <td>624750v1</td>\n",
       "      <td>https://www.ralphlauren.nl/en/double-knit-pull...</td>\n",
       "      <td>[Army Olive, Aviator Navy, Montana Khaki]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Snowflake Estate-Rib Jumper</td>\n",
       "      <td>590613v1</td>\n",
       "      <td>https://www.ralphlauren.nl/en/snowflake-estate...</td>\n",
       "      <td>[Heritage Snowflake New Fo, Heritage Snowflake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Colour-Blocked Stretch Jersey Pullover</td>\n",
       "      <td>625356</td>\n",
       "      <td>https://www.ralphlauren.nl/en/colour-blocked-s...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Stretch Jersey Half-Zip Pullover</td>\n",
       "      <td>639982</td>\n",
       "      <td>https://www.ralphlauren.nl/en/stretch-jersey-h...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>Cotton-Blend-Fleece Hoodie</td>\n",
       "      <td>458755</td>\n",
       "      <td>https://www.ralphlauren.nl/en/cotton-blend-fle...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>Polo Bear Color-Blocked Fleece Hoodie</td>\n",
       "      <td>647816</td>\n",
       "      <td>https://www.ralphlauren.nl/en/polo-bear-color-...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Classic Fit Camo Jacquard Pullover</td>\n",
       "      <td>625360</td>\n",
       "      <td>https://www.ralphlauren.nl/en/classic-fit-camo...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>274 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title data-itemid  \\\n",
       "0     Triple-Pony Plaid-Print Fleece Hoodie      624792   \n",
       "1                    Polo Ski Fleece Hoodie      624775   \n",
       "2                      The RL Fleece Hoodie    490877v1   \n",
       "3                      Double-Knit Pullover    624750v1   \n",
       "4               Snowflake Estate-Rib Jumper    590613v1   \n",
       "..                                      ...         ...   \n",
       "269  Colour-Blocked Stretch Jersey Pullover      625356   \n",
       "270        Stretch Jersey Half-Zip Pullover      639982   \n",
       "271              Cotton-Blend-Fleece Hoodie      458755   \n",
       "272   Polo Bear Color-Blocked Fleece Hoodie      647816   \n",
       "273      Classic Fit Camo Jacquard Pullover      625360   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://www.ralphlauren.nl/en/triple-pony-plai...   \n",
       "1    https://www.ralphlauren.nl/en/polo-ski-fleece-...   \n",
       "2    https://www.ralphlauren.nl/en/the-rl-fleece-ho...   \n",
       "3    https://www.ralphlauren.nl/en/double-knit-pull...   \n",
       "4    https://www.ralphlauren.nl/en/snowflake-estate...   \n",
       "..                                                 ...   \n",
       "269  https://www.ralphlauren.nl/en/colour-blocked-s...   \n",
       "270  https://www.ralphlauren.nl/en/stretch-jersey-h...   \n",
       "271  https://www.ralphlauren.nl/en/cotton-blend-fle...   \n",
       "272  https://www.ralphlauren.nl/en/polo-bear-color-...   \n",
       "273  https://www.ralphlauren.nl/en/classic-fit-camo...   \n",
       "\n",
       "                                                colors  \n",
       "0                                                   []  \n",
       "1                                                   []  \n",
       "2         [Holiday Red, Athletic Green, Harrison Blue]  \n",
       "3            [Army Olive, Aviator Navy, Montana Khaki]  \n",
       "4    [Heritage Snowflake New Fo, Heritage Snowflake...  \n",
       "..                                                 ...  \n",
       "269                                                 []  \n",
       "270                                                 []  \n",
       "271                                                 []  \n",
       "272                                                 []  \n",
       "273                                                 []  \n",
       "\n",
       "[274 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3c623f6acdd19ab33df0479ab84ef2ba5ed62403b128c17015aa00765b31ae5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
